{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a2fb5ed-9a79-47a6-8031-be23ef774259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6230164-720c-4b87-aa01-b56f935c72ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = models.vgg11()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1bd7e53-5032-4bc8-81ab-c3d265722ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.classifier[6] = nn.Linear(in_features=4096, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4b04acd-96c4-47d5-a979-23938b94af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.classifier[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb211699-b821-40cb-9115-ecc788587b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inc_CNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(Inc_CNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        # self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        # self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.cnn = models.vgg16(pretrained=True)\n",
    "        self.cnn.classifier[6] = nn.Linear(in_features=4096,out_features=embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn(images)\n",
    "        return self.dropout(self.relu(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10180e2f-7a83-466b-af83-56b9a3be4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_c = Inc_CNN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a52de569-aa03-4594-a881-bbdd9b33afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand((1,3,244,244))\n",
    "# print(x.shape)\n",
    "# print(x)\n",
    "# enc_c(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6b03dfb-e54d-441c-b73e-74c036131ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_RNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(Custom_RNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l_softmax = nn.LogSoftmax((hidden_size,vocab_size))\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        print('emb',embeddings.shape)\n",
    "        inp = embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        # print('emmb cat:', embeddings.shape)\n",
    "        embeddings = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted= False)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        # h_0 = features.unsqueeze(0).float()\n",
    "        # c_0 = torch.rand(1,features.shape[-2],features.shape[-1])\n",
    "        # print(h_0.shape)\n",
    "        # print(c_0.shape)\n",
    "        # hiddens, _ = self.lstm(inp, (h_0, c_0))\n",
    "        # hiddens = torch.tensor(hiddens)\n",
    "        # print('h\\'s sape',hiddens[0].shape)\n",
    "        # print('h\\'s sape',hiddens[1].shape)\n",
    "        # print('h\\'s sape',hiddens[2].shape)\n",
    "        # print('h\\'s sape',hiddens[3].shape)\n",
    "        # print('h\\'s sape',len(hiddens))\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        # outputs = self.l_softmax(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75ee55f3-9a2c-4649-a88c-a9efc0f3d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = Custom_RNN(10, 10, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73125c1a-6380-4aca-8271-6abe0c1b885c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = torch.LongTensor([[1, 2, 2, 0, 1, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 9, 0, 1, 0, 0, 5, 0, 3],\n",
    "                        [8, 9, 0, 0, 1, 0, 0, 0, 4, 0],\n",
    "                        [9, 8, 1, 0, 5, 0, 0, 7, 0, 0]])\n",
    "fet = torch.LongTensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
    "                        [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
    "                        [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
    "                        [0, 1, 1, 0, 0, 0, 0, 0, 0, 1]])\n",
    "# print('cap:', cap)\n",
    "# print('fet:', fet)\n",
    "# print('unsqz', fet.unsqueeze(0))\n",
    "# cap = pack_padded_sequence(cap, [10], batch_first=True, enforce_sorted=False)\n",
    "result = crnn(fet, cap, [5, 5, 5, 5])\n",
    "print('Result:',result.shape)\n",
    "# print(result[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a584d2f-2430-4f1f-a9d1-9163d8c8a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = torch.LongTensor([[1, 2, 2, 4, 1, 0, 1, 1, 1, 1]])\n",
    "# fet = torch.LongTensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 1]])\n",
    "# # print('cap:', cap)\n",
    "# # print('fet:', fet)\n",
    "# # print('unsqz', fet.unsqueeze(0))\n",
    "# # cap = pack_padded_sequence(cap, [10], batch_first=True, enforce_sorted=False)\n",
    "# result = crnn(fet, cap, [10])\n",
    "# print('Result:',result.shape)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ef92b2-dca5-4723-8871-c6a716bb3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(RCNN, self).__init__()\n",
    "        self.encoderCNN = Inc_CNN(embed_size)\n",
    "        self.decoderRNN = Custom_RNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        for params in self.encoderCNN.parameters():\n",
    "            params.require_grad = False\n",
    "        for params in self.encoderCNN.cnn.classifier[6].parameters():\n",
    "            params.require_grad = True\n",
    "\n",
    "    def forward(self, images, captions, lenghts):\n",
    "        features = self.encoderCNN(images)\n",
    "        #print('done for encoder')\n",
    "        # mioght need to change type of features\n",
    "        outputs = self.decoderRNN(features, captions, lenghts)\n",
    "        #print('done for decoder')\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]\n",
    "    \n",
    "    def get_bs_pred(self, features, hidden=None):\n",
    "        ''' Helper Function for Beam Search'''\n",
    "        if(hidden != None):\n",
    "            features = self.embed(features).unsqueeze(1)\n",
    "        output, hidden = self.lstm(features, hidden)\n",
    "        output = self.linear(output.squeeze(1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "280b43b0-0d95-42fe-9b87-793c531f2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 2\n",
    "hidden_size = 2\n",
    "vocab_size = 10\n",
    "layers = 1\n",
    "rcnn_local = RCNN(embed_size=embed_size,hidden_size=hidden_size, vocab_size=vocab_size, num_layers=layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659df093-00a3-420a-98a9-60800914ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption_l = torch.tensor([[0,1,2,1,0]])\n",
    "# print(type(caption_l))\n",
    "# op = rcnn_local(x, caption_l)\n",
    "# print('Output shape',op.shape)\n",
    "# print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef26eb-deb3-4a8b-9f3a-3572a24f7aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
