{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e58934e-6be9-497b-a63c-b8a3dda95e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from skimage import io, transform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da15cfb0-0f89-44dc-8fe1-a94b0d8fb321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Model_non_comp.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "'''Local Improts'''\n",
    "from Model_non_comp import RCNN\n",
    "from Vocab_class import TermVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6475711-b052-446e-9632-4f2cebe83864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        #print('in transform init, rescalling')\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        #print('in transform call')\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        #print('in transform to tensor call')\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return image\n",
    "\n",
    "\n",
    "# IMAGE_RESIZE = (256, 256)\n",
    "\n",
    "# # For inception V3\n",
    "# IMAGE_RESIZE = (299, 299)\n",
    "\n",
    "# #For VGG 16\n",
    "# IMAGE_RESIZE = (244, 244)\n",
    "\n",
    "#For VGG 19\n",
    "IMAGE_RESIZE = (224, 224)\n",
    "\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([\n",
    "    Rescale(IMAGE_RESIZE), ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c017ee-0f9d-42ed-a324-ade901d5435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vocab\n"
     ]
    }
   ],
   "source": [
    "class CaptionsPreprocessing:\n",
    "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "\n",
    "    Args:\n",
    "        captions_file_path (string): captions tsv file path\n",
    "    \"\"\"\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.longest_seq = 0\n",
    "        \n",
    "        self.captions_file_path = captions_file_path\n",
    "\n",
    "        # Read raw captions\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "\n",
    "        # Create vocabulary\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "        \n",
    "\n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[img_captions[0]] = img_captions[1]\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict\n",
    "        longest_seq = 0\n",
    "        start = ['<start>']\n",
    "        end   = ['<end>']\n",
    "        pos   = ['<pos>']\n",
    "        # Do the preprocessing here\n",
    "        for img_id in raw_captions_dict:\n",
    "            term_list = raw_captions_dict[img_id].split()\n",
    "            # Add nikki's code\n",
    "            # term_list = \n",
    "            raw_captions_dict[img_id] = term_list\n",
    "            if len(term_list)> longest_seq:\n",
    "                longest_seq = len(term_list)\n",
    "        \n",
    "        self.longest_seq = longest_seq+2\n",
    "        for img_id in raw_captions_dict:\n",
    "            term_list = raw_captions_dict[img_id]\n",
    "            term_list = start + term_list + end\n",
    "            l = len(term_list)\n",
    "            if l < longest_seq + 2:\n",
    "                term_list = term_list[:-1]+pos*(longest_seq+2 - l)+ end\n",
    "            raw_captions_dict[img_id] = term_list\n",
    "                \n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        captions_dict = self.captions_dict\n",
    "\n",
    "        # Generate the vocabulary\n",
    "        vocab = TermVocab()\n",
    "        #longest_term = 0\n",
    "        #vocab = {}\n",
    "        #index = \n",
    "        for img_id in captions_dict:\n",
    "            contents = captions_dict[img_id]#.split()\n",
    "            #print('for img-',img_id,\":\",contents)\n",
    "            for term in contents:\n",
    "                vocab.add_term(term)\n",
    "        print('Generated vocab')\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "    def captions_transform(self, img_caption_list):\n",
    "        \"\"\"\n",
    "        Use this function to generate tensor tokens for the text captions\n",
    "        Args:\n",
    "            img_caption_list: List of captions for a particular image\n",
    "        \"\"\"\n",
    "        vocab = self.vocab\n",
    "\n",
    "        # Generate tensors\n",
    "        #print(\"caption transform called, with img caption list:\", img_caption_list)\n",
    "        #print(img_caption_list[0])\n",
    "        img_cap_terms = img_caption_list\n",
    "        op = torch.zeros(self.longest_seq)\n",
    "        #print(self.longest_seq)\n",
    "        for i, term in enumerate(img_cap_terms):\n",
    "            #print('term:', term,i)\n",
    "            op[i] = vocab.to_index(term)\n",
    "        #print('caption transform op:', op)\n",
    "        return op\n",
    "        # OLD:return torch.zeros(len(img_caption_list), 10)\n",
    "\n",
    "# Set the captions tsv file path\n",
    "CAPTIONS_FILE_PATH = '/media/harsh/Common/IITD/COL774-ML/ass4/Train_text.tsv'\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85bfdef-91f0-43b2-b9a7-59c145672114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'men'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_preprocessing_obj.vocab.to_term(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c1864b8-04dd-44e5-a11a-1f19177fa425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([73.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_preprocessing_obj.captions_transform(['one','two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d45d29-b2d2-436b-9947-e2e3a7f6aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = img_transform\n",
    "        #print('img transform',img_transform)\n",
    "        self.captions_transform = captions_transform\n",
    "\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_ids[idx]\n",
    "        #print('img name', img_name)\n",
    "        IMAGE_DIR = '/media/harsh/Common/IITD/COL774-ML/ass4/train_data/'\n",
    "        image = io.imread(IMAGE_DIR+img_name)\n",
    "        captions = self.captions_dict[img_name]\n",
    "\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "\n",
    "        if self.captions_transform:\n",
    "            captions = self.captions_transform(captions)\n",
    "\n",
    "        sample = {'image': image, 'captions': captions}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa16d633-b90e-4032-9137-e3b5cd67d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 0.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score - log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50d66bc-79db-4231-943f-081ea382a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]\n"
     ]
    }
   ],
   "source": [
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "'''\n",
    "data = 10 words in a vocab of size 5 \n",
    "\n",
    "'''\n",
    "result = beam_search_decoder(np.array(data), 3)\n",
    "for seq in result:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5324d-6f92-4baa-971f-336f7d98e29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20f2e38d-fe96-459f-8f98-3956fa082db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7738\n",
      "10\n",
      "<end>\n",
      "<pos>\n"
     ]
    }
   ],
   "source": [
    "print(captions_preprocessing_obj.vocab.vocab_length)\n",
    "print(captions_preprocessing_obj.longest_seq)\n",
    "print(captions_preprocessing_obj.vocab.to_term(7))\n",
    "print(captions_preprocessing_obj.vocab.to_term(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb7d851-a986-4262-973c-9343d0095e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = captions_preprocessing_obj.longest_seq\n",
    "hidden_size = 20\n",
    "vocab_size = captions_preprocessing_obj.vocab.vocab_length\n",
    "layers = captions_preprocessing_obj.longest_seq\n",
    "\n",
    "rcnn = RCNN(embed_size=embed_size,hidden_size=hidden_size, vocab_size=vocab_size, num_layers=layers)\n",
    "# \n",
    "# torch.save(rcnn.state_dict(), 'model_weights_ncomp.pth')\n",
    "# rcnn.float()\n",
    "# For GPU Training\n",
    "# rcnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02444d70-2707-43a9-b935-6e88d8e70524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(rcnn, 'rncc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "900edbcd-61ef-486b-8486-4af40ba67da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# output = loss(input, target)\n",
    "# output.backward()\n",
    "# Example of target with class probabilities\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.randn(3, 5).softmax(dim=1)\n",
    "# output = loss(input, target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b65b5bc0-c9b3-427f-9a63-b9848055c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_op(prediction):\n",
    "    vocab = captions_preprocessing_obj.vocab\n",
    "    caption = [vocab.to_term(term_idx) for term_idx in prediction]\n",
    "    print(caption)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba5a7b-b3b5-4e6c-becd-91cb905185f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0dcb86-db26-4dd3-afa1-e939861f5042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16cfe09f0494c45adec8b4b31cd32b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.1748, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1627336343171/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.3688, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.3921, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(2.9579, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.0878, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.4172, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.3269, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.5227, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.1280, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.1716, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.1180, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.1301, grad_fn=<NllLossBackward>)\n",
      "emb torch.Size([32, 10, 10])\n",
      "loss: tensor(3.3123, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99135/2444064868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# avg_loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# loss = loss_function(output_captions[0], captions_batch[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_weights_ncomp.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "continue_training = True\n",
    "IMAGE_DIR = '/media/harsh/Common/IITD/COL774-ML/ass4/train_data/'\n",
    "# Creating the Dataset\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "# rcnn = None\n",
    "# if continue_training == True:\n",
    "#     rcnn = torch.load('rcnn.pth')\n",
    "rcnn.load_state_dict(torch.load('model_weights_ncomp.pth'))\n",
    "# Define your hyperparameters\n",
    "NUMBER_OF_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-1\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "continue_gtrain = True\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(rcnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#Other parameters\n",
    "caption_length = captions_preprocessing_obj.longest_seq\n",
    "vocab_size = captions_preprocessing_obj.vocab.vocab_length\n",
    "# Creating the DataLoader for batching purposes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "import os\n",
    "soft_m = nn.Softmax()\n",
    "#print(rcnn)\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, sample in tqdm(enumerate(train_loader)):\n",
    "        rcnn.zero_grad()\n",
    "\n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "        image_batch    = image_batch.float()\n",
    "        captions_batch = captions_batch.long()\n",
    "        lengths = [caption_length]*BATCH_SIZE\n",
    "        target_rasha = pack_padded_sequence(captions_batch, lengths, batch_first=True, enforce_sorted =False)[0]\n",
    "        # If GPU training required\n",
    "        # image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
    "        \n",
    "        # print(image_batch.shape, captions_batch.shape)\n",
    "        # print(captions_batch[0].dtype)\n",
    "        # print(image_batch.dtype)\n",
    "        #print(image_batch[0])\n",
    "        output_captions = rcnn(image_batch, captions_batch, lengths)\n",
    "        loss = loss_function(output_captions, target_rasha)\n",
    "        # print('computed loss baby')\n",
    "        print('loss:',loss)\n",
    "        #output_captions = torch.reshape(output_captions, (BATCH_SIZE, -1, vocab_size))\n",
    "        # print('output size:',output_captions.shape)\n",
    "        # print('real caption size:',captions_batch.shape)\n",
    "        \n",
    "        # avg_loss = torch.tensor(0.0)\n",
    "        # for b in range(BATCH_SIZE):\n",
    "        #     # print('Target ind:',captions_batch[b])\n",
    "        #     # print('Target seq:',process_op(captions_batch[b].tolist()))\n",
    "        #     #avg_loss += loss_function(output_captions[b], captions_batch[b])\n",
    "        #     pred = beam_search_decoder(soft_m(output_captions[b]), 1)\n",
    "        #     # print(len(pred[0][0]))\n",
    "        #     print('Prediction ind:',pred[0][0])\n",
    "        #     print('Prediction seq:',process_op(pred[0][0]))\n",
    "            \n",
    "        # avg_loss /= BATCH_SIZE\n",
    "        # print('avg loss:',avg_loss)\n",
    "        # avg_loss.backward()\n",
    "        # loss = loss_function(output_captions[0], captions_batch[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.save(rcnn.state_dict(), 'model_weights_ncomp.pth')\n",
    "    print(\"Iteration: \" + str(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907349b-9a55-4cc4-9187-dc3d9b6fca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
